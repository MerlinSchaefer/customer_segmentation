{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a626959-61c8-4bba-84d2-2a4ecab1f7ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DLT pipeline for training data\n",
    "\n",
    "This Delta Live Tables (DLT) definition is executed using a pipeline defined in resources/customer_segmentation_dlt.yml. It contains the DLT Steps for creating valid training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9198e987-5606-403d-9f6d-8f14e6a4017f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, when, current_timestamp\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "from dlt_utils import load_schema_from_yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9c32c07-452d-44a2-aaa3-dc5881f85f2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve settings from the Spark configuration (set in the YAML)\n",
    "schema_path = spark.conf.get(\"bronze_schema_path\")\n",
    "bronze_schema_name = spark.conf.get(\"bronze_schema_name\")\n",
    "input_path = spark.conf.get(\"cloudFiles.inputPath\")\n",
    "file_format = spark.conf.get(\"cloudFiles.format\")\n",
    "max_files_per_trigger = spark.conf.get(\"cloudFiles.maxFilesPerTrigger\")\n",
    "header_option = spark.conf.get(\"cloudFiles.header\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7baed510-1156-47c9-b29f-b77a5922d18e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_data_schema = load_schema_from_yaml(schema_path, bronze_schema_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc19dba-61fd-4a89-8f8c-24fee63bfb14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Bronze Table: Raw data ingestion from DBFS using Auto Loader\n",
    "\n",
    "@dlt.table\n",
    "def bronze_training_customer_data():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")  # Use Auto Loader\n",
    "        .option(\"cloudFiles.format\", file_format)\n",
    "        .option(\"cloudFiles.header\", header_option)  # Ensure header is recognized\n",
    "        .option(\"cloudFiles.maxFilesPerTrigger\", max_files_per_trigger)\n",
    "        .schema(raw_data_schema)\n",
    "        .load(input_path)  # Path and other options are configured in the YAML\n",
    "        .withColumn(\"ingest_timestamp\", current_timestamp())  # Add ingest timestamp\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28e3d104-9b41-4fff-963a-7dec5efd9102",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Silver Table: Cleaned and transformed data\n",
    "@dlt.table(\n",
    "    comment = \"silver table with valid customer data for training\"\n",
    ")\n",
    "@dlt.expect_all_or_drop({\n",
    "    \"valid_id\": \"id IS NOT NULL\",  # Ensure ID is not null\n",
    "    \"valid_segmentation\": \"segmentation IS NOT NULL AND segmentation IS IN ('A', 'B', 'C', 'D')\"  # Ensure Segmentation (target) is not null\n",
    "    \"non_negative_age\": \"age >= 0\",\n",
    "    \"valid_age\": \"age <= 120\",\"\n",
    "    \"valid_family_size\": \"family_size >= 0 AND family_size <= 15\",\n",
    "    \"valid_work_experience\": \"work_experience >= 0 AND work_experience <= 50\"\n",
    "})\n",
    "def silver_training_customer_data():\n",
    "    bronze_df = dlt.read(\"bronze_training_customer_data\")\n",
    "    # Standardize column names (lowercase, underscores instead of spaces)\n",
    "    df = bronze_df.toDF(*[col.lower().replace(' ', '_') for col in bronze_df.columns])\n",
    "\n",
    "    # Convert Age and Family Size to integers, Work Experience to float\n",
    "    df = (df.withColumn(\"age\", col(\"age\").cast(\"int\"))\n",
    "          .withColumn(\"family_size\", col(\"family_size\").cast(\"int\"))\n",
    "          .withColumn(\"work_experience\", col(\"work_experience\").cast(\"float\"))\n",
    "         )\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d41cf623-5a0b-4047-8335-3bd56bc81dd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define categorical columns for mode imputation\n",
    "categorical_columns_custom = [\"ever_married\", \"graduated\", \"gender\", \"spending_score\", 'profession', 'var_1']\n",
    "\n",
    "# Define numerical columns for median imputation\n",
    "numerical_columns = ['age', 'family_size', 'work_experience'] \n",
    "\n",
    "# Define the columns for encoding\n",
    "categorical_columns_onehot = ['profession', 'var_1']\n",
    "categorical_columns_ordinal = ['spending_score']\n",
    "categorical_columns_custom = [\"ever_married\", \"graduated\", \"gender\"]\n",
    "\n",
    "# Custom mapping for categorical columns\n",
    "custom_mapping = [\n",
    "    ({'Yes': 1, 'No': 0}, [\"ever_married\", \"graduated\"]),\n",
    "    ({'Male': 1, 'Female': 0}, [\"gender\"]),\n",
    "    ({'Low': 0, 'Average': 1, 'High': 2}, [\"spending_score\"])\n",
    "]\n",
    "custom_mapping_target = {\"A\":1, \"B\":2, \"C\":3, \"D\":4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "475d3abb-3000-4ff7-b72b-89eb9c021238",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to generate a DataFrame for each mapping\n",
    "def create_lookup_df(mapping, column_name):\n",
    "    lookup_list = [(k, v) for k, v in mapping.items()]\n",
    "    return spark.createDataFrame(lookup_list, schema=[\"string_value\", \"numeric_value\"]).withColumn(\"column\", F.lit(column_name))\n",
    "\n",
    "@dlt.table\n",
    "def gold_mapping_lookup():\n",
    "    # Create an empty DataFrame to store all lookup mappings\n",
    "    lookup_df = None\n",
    "    \n",
    "    # Iterate through custom mappings and create a DataFrame for each, then union them\n",
    "    for mapping, columns in custom_mapping:\n",
    "        for column in columns:\n",
    "            if lookup_df is None:\n",
    "                lookup_df = create_lookup_df(mapping, column)\n",
    "            else:\n",
    "                lookup_df = lookup_df.union(create_lookup_df(mapping, column))\n",
    "    \n",
    "    return lookup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85b76201-cf57-4f1b-98df-bb211c2b6971",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gold Table: Business-ready or aggregated data\n",
    "@dlt.table\n",
    "def gold_customer_features():\n",
    "    silver_df = dlt.read(\"silver_training_customer_data\")\n",
    "    gold_df = silver_df.drop(\"id\", \"segmentation\",\"inserted_at\")\n",
    "    # custom imputation for missing values of profession and ever_married\n",
    "    gold_df.fillna(\"Other\", subset=[\"profession\"]).fillna(\"No\", subset=[\"ever_married\"])\n",
    "    \n",
    "    # encoding of categorical variables\n",
    "    # Custom encoding for Ever_Married, Graduated, Gender, and Spending_Score\n",
    "    for mapping, columns in custom_mapping:\n",
    "        for column in columns:\n",
    "            gold_df = gold_df.withColumn(column, \n",
    "                F.when(F.col(column).isin(mapping.keys()), \n",
    "                       F.create_map([F.lit(k), F.lit(v) for k, v in mapping.items()])[F.col(column)]\n",
    "                      ).otherwise(F.lit(None)))  # Handle invalid values by setting to None\n",
    "    # Mode Imputation for all categorical columns\n",
    "    for column in categorical_columns_custom:\n",
    "        mode_value = df.groupBy(column).count().orderBy(F.desc(\"count\")).first()[0]  # Calculate mode\n",
    "        df = df.withColumn(column, F.when(F.col(column).isNull(), F.lit(mode_value)).otherwise(F.col(column)))\n",
    "    # Median Imputation for all numerical columns\n",
    "    for column in numerical_columns:\n",
    "        median_value = df.approxQuantile(column, [0.5], 0.01)[0]  # Calculate median\n",
    "        df = df.withColumn(column, F.when(F.col(column).isNull(), F.lit(median_value)).otherwise(F.col(column)))\n",
    "\n",
    "    return gold_df\n",
    "\n",
    "@dlt.table\n",
    "def gold_customer_ml_target():\n",
    "    gold_df_target = dlt.read(\"silver_training_customer_data\").select(\"segmentation\")\n",
    "    gold_df_target = gold_df_target.withColumn(\"segmentation\", \n",
    "                       F.when(F.col(\"segmentation\") == \"A\", segmentation_mapping[\"A\"])\n",
    "                        .when(F.col(\"segmentation\") == \"B\", segmentation_mapping[\"B\"])\n",
    "                        .when(F.col(\"segmentation\") == \"C\", segmentation_mapping[\"C\"])\n",
    "                        .otherwise(segmentation_mapping[\"D\"])\n",
    "    )\n",
    "    return gold_df_target\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dlt_pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
