{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a626959-61c8-4bba-84d2-2a4ecab1f7ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DLT pipeline\n",
    "\n",
    "This Delta Live Tables (DLT) definition is executed using a pipeline defined in resources/customer_segmentation_dlt.yml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9198e987-5606-403d-9f6d-8f14e6a4017f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7baed510-1156-47c9-b29f-b77a5922d18e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: define schema\n",
    "raw_data_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    # Add more fields as necessary\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc19dba-61fd-4a89-8f8c-24fee63bfb14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Bronze Table: Raw data ingestion from DBFS using Auto Loader\n",
    "\n",
    "@dlt.table\n",
    "def bronze_customer_data():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")  # Use Auto Loader\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.header\", \"true\")  # Ensure header is recognized\n",
    "        .schema(raw_data_schema)\n",
    "        .load()  # Path and other options are configured in the YAML\n",
    "        .withColumn(\"ingest_timestamp\", current_timestamp())  # Add ingest timestamp\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "# Silver Table: Cleaned and transformed data\n",
    "@dlt.table\n",
    "def silver_customer_data():\n",
    "    bronze_df = dlt.read(\"bronze_customer_data\")\n",
    "    return bronze_df.filter(col(\"customer_id\").isNotNull())  # Example: filter out rows with null customer IDs\n",
    "\n",
    "# Gold Table: Business-ready or aggregated data\n",
    "@dlt.table\n",
    "def gold_customer_aggregations():\n",
    "    silver_df = dlt.read(\"silver_customer_data\")\n",
    "    return silver_df.groupBy(\"customer_segment\").agg({\"spend\": \"sum\"})  # Example: group by customer segment and aggregate spend\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dlt_pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
