{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a626959-61c8-4bba-84d2-2a4ecab1f7ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DLT pipeline for training data\n",
    "\n",
    "This Delta Live Tables (DLT) definition is executed using a pipeline defined in resources/customer_segmentation_dlt.yml. It contains the DLT Steps for creating valid training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9198e987-5606-403d-9f6d-8f14e6a4017f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7baed510-1156-47c9-b29f-b77a5922d18e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: define schema\n",
    "raw_data_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    # Add more fields as necessary\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc19dba-61fd-4a89-8f8c-24fee63bfb14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Bronze Table: Raw data ingestion from DBFS using Auto Loader\n",
    "\n",
    "@dlt.table\n",
    "def bronze_training_customer_data():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")  # Use Auto Loader\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.header\", \"true\")  # Ensure header is recognized\n",
    "        .schema(raw_data_schema)\n",
    "        .load()  # Path and other options are configured in the YAML\n",
    "        .withColumn(\"ingest_timestamp\", current_timestamp())  # Add ingest timestamp\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28e3d104-9b41-4fff-963a-7dec5efd9102",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Silver Table: Cleaned and transformed data\n",
    "@dlt.table(\n",
    "    comment = \"silver table with valid customer data for training\"\n",
    ")\n",
    "@dlt.expect_all_or_drop({\n",
    "    \"valid_id\": \"id IS NOT NULL\",  # Ensure ID is not null\n",
    "    \"valid_segmentation\": \"segmentation IS NOT NULL\"  # Ensure Segmentation (target) is not null\n",
    "})\n",
    "@dlt.expect_all({\n",
    "    \"non_negative_age\": \"age >= 0\",\n",
    "    \"valid_age\": \"age <= 120\",\"\n",
    "    \"valid_family_size\": \"family_size >= 0 AND family_size <= 15\",\n",
    "    \"valid_work_experience\": \"work_experience >= 0 AND work_experience <= 50\"\n",
    "})\n",
    "def silver_training_customer_data():\n",
    "    bronze_df = dlt.read(\"bronze_training_customer_data\")\n",
    "    # Standardize column names (lowercase, underscores instead of spaces)\n",
    "    df = bronze_df.toDF(*[col.lower().replace(' ', '_') for col in bronze_df.columns])\n",
    "\n",
    "    # Convert Yes/No columns to binary (e.g., Ever_Married, Graduated)\n",
    "    df = (df.withColumn(\"ever_married\", when(col(\"ever_married\") == \"Yes\", 1).otherwise(0))\n",
    "          .withColumn(\"graduated\", when(col(\"graduated\") == \"Yes\", 1).otherwise(0))\n",
    "        )\n",
    "\n",
    "    # Convert Age and Family Size to integers, Work Experience to float\n",
    "    df = (df.withColumn(\"age\", col(\"age\").cast(\"int\"))\n",
    "          .withColumn(\"family_size\", col(\"family_size\").cast(\"int\"))\n",
    "          .withColumn(\"work_experience\", col(\"work_experience\").cast(\"float\"))\n",
    "         )\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85b76201-cf57-4f1b-98df-bb211c2b6971",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gold Table: Business-ready or aggregated data\n",
    "@dlt.table\n",
    "def gold_customer_aggregations():\n",
    "    silver_df = dlt.read(\"silver_training_customer_data\")\n",
    "    return silver_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dlt_pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
