{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a626959-61c8-4bba-84d2-2a4ecab1f7ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DLT pipeline for training data\n",
    "\n",
    "This Delta Live Tables (DLT) definition is executed using a pipeline defined in resources/customer_segmentation_dlt.yml. It contains the DLT Steps for creating valid training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9198e987-5606-403d-9f6d-8f14e6a4017f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7baed510-1156-47c9-b29f-b77a5922d18e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: define schema\n",
    "raw_data_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    # Add more fields as necessary\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc19dba-61fd-4a89-8f8c-24fee63bfb14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Bronze Table: Raw data ingestion from DBFS using Auto Loader\n",
    "\n",
    "@dlt.table\n",
    "def bronze_training_customer_data():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")  # Use Auto Loader\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.header\", \"true\")  # Ensure header is recognized\n",
    "        .schema(raw_data_schema)\n",
    "        .load()  # Path and other options are configured in the YAML\n",
    "        .withColumn(\"ingest_timestamp\", current_timestamp())  # Add ingest timestamp\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28e3d104-9b41-4fff-963a-7dec5efd9102",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Silver Table: Cleaned and transformed data\n",
    "@dlt.table(\n",
    "    comment = \"silver table with valid customer data for training\"\n",
    ")\n",
    "@dlt.expect_all_or_drop({\n",
    "    \"valid_id\": \"id IS NOT NULL\",  # Ensure ID is not null\n",
    "    \"valid_segmentation\": \"segmentation IS NOT NULL\"  # Ensure Segmentation (target) is not null\n",
    "})\n",
    "@dlt.expect_all({\n",
    "    \"non_negative_age\": \"age >= 0\",\n",
    "    \"valid_age\": \"age <= 120\",\"\n",
    "    \"valid_family_size\": \"family_size >= 0 AND family_size <= 15\",\n",
    "    \"valid_work_experience\": \"work_experience >= 0 AND work_experience <= 50\"\n",
    "})\n",
    "def silver_training_customer_data():\n",
    "    bronze_df = dlt.read(\"bronze_training_customer_data\")\n",
    "    # Standardize column names (lowercase, underscores instead of spaces)\n",
    "    df = bronze_df.toDF(*[col.lower().replace(' ', '_') for col in bronze_df.columns])\n",
    "\n",
    "    # Convert Yes/No columns to binary (e.g., Ever_Married, Graduated)\n",
    "    df = (df.withColumn(\"ever_married\", when(col(\"ever_married\") == \"Yes\", 1).otherwise(0))\n",
    "          .withColumn(\"graduated\", when(col(\"graduated\") == \"Yes\", 1).otherwise(0))\n",
    "        )\n",
    "\n",
    "    # Convert Age and Family Size to integers, Work Experience to float\n",
    "    df = (df.withColumn(\"age\", col(\"age\").cast(\"int\"))\n",
    "          .withColumn(\"family_size\", col(\"family_size\").cast(\"int\"))\n",
    "          .withColumn(\"work_experience\", col(\"work_experience\").cast(\"float\"))\n",
    "         )\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d41cf623-5a0b-4047-8335-3bd56bc81dd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the columns\n",
    "categorical_columns_onehot = ['profession', 'var_1']\n",
    "categorical_columns_ordinal = ['spending_score']\n",
    "categorical_columns_custom = [\"ever_married\", \"graduated\", \"gender\"]\n",
    "\n",
    "# Custom mapping for categorical columns\n",
    "custom_mapping = [\n",
    "    ({'Yes': 1, 'No': 0}, [\"ever_married\", \"graduated\"]),\n",
    "    ({'Male': 1, 'Female': 0}, [\"gender\"])\n",
    "]\n",
    "# ordinal mapping for spending_score\n",
    "ordinal_mapping = {'Low': 0, 'Average': 1, 'High': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85b76201-cf57-4f1b-98df-bb211c2b6971",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gold Table: Business-ready or aggregated data\n",
    "@dlt.table\n",
    "def gold_customer_ml_features():\n",
    "    silver_df = dlt.read(\"silver_training_customer_data\")\n",
    "    gold_df = silver_df.drop(\"id\", \"segmentation\",\"inserted_at\")\n",
    "    # simple imputation for missing values\n",
    "    gold_df.fillna(\"Other\", subset=[\"profession\"]).fillna(\"No\", subset=[\"ever_married\"])\n",
    "    # encoding of categorical variables\n",
    "    # Custom encoding for Ever_Married, Graduated, and Gender\n",
    "    for mapping, columns in custom_mapping:\n",
    "        for column in columns:\n",
    "            df = df.withColumn(column, \n",
    "                               F.when(F.col(column).isin(mapping.keys()), \n",
    "                                        F.create_map([F.lit(k), F.lit(v) for k, v in mapping.items()])[F.col(column)]).otherwise(F.col(column))\n",
    "                               )\n",
    "\n",
    "    # OneHot Encoding for 'profession' and 'var_1'\n",
    "    for column in categorical_columns_onehot:\n",
    "        indexer = StringIndexer(inputCol=column, outputCol=f\"{column}_index\")\n",
    "        df = indexer.fit(df).transform(df)\n",
    "        \n",
    "        encoder = OneHotEncoder(inputCol=f\"{column}_index\", outputCol=f\"{column}_onehot\")\n",
    "        df = encoder.fit(df).transform(df)\n",
    "    \n",
    "\n",
    "    df = df.withColumn(\n",
    "                        'spending_score', \n",
    "                        F.when(F.col('spending_score') == 'Low', ordinal_mapping['Low'])\n",
    "                         .when(F.col('spending_score') == 'Average', ordinal_mapping['Average'])\n",
    "                         .when(F.col('spending_score') == 'High', ordinal_mapping['High'])\n",
    "                         .otherwise(F.lit(None))\n",
    "    )\n",
    "\n",
    "    # Drop unnecessary columns (e.g., original categorical columns after encoding)\n",
    "    df = df.drop(*categorical_columns_onehot)\n",
    "\n",
    "    return silver_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dlt_pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
